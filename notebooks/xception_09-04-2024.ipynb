{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabu\\AppData\\Local\\Temp\\ipykernel_19328\\1587534397.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle #for saving history object\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from datetime import datetime\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'D:/testproject/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
    "val_dir = 'D:/testproject/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
    "test_dir = 'D:/testproject/New Plant Diseases Dataset(Augmented)/test/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load image file paths and corresponding labels into a DataFrame\n",
    "def load_data(directory):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for class_label in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_label)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            file_paths.append(os.path.join(class_dir, img_file))\n",
    "            labels.append(class_label)\n",
    "    return file_paths, labels\n",
    "\n",
    "train_file_paths, train_labels = load_data(train_dir)\n",
    "val_file_paths, val_labels = load_data(val_dir)\n",
    "\n",
    "train_df = pd.DataFrame({'File_Path': train_file_paths, 'Label': train_labels})\n",
    "val_df = pd.DataFrame({'File_Path': val_file_paths, 'Label': val_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the DataFrame to CSV files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.02 MiB for an array with shape (299, 299, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load and preprocess images for train and validation sets\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_image(file_path) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile_Path\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     10\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_image(file_path) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile_Path\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     11\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load and preprocess images for train and validation sets\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile_Path\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     10\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_image(file_path) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile_Path\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     11\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(file_path):\n\u001b[0;32m      3\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_img(file_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m299\u001b[39m, \u001b[38;5;241m299\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m \u001b[43mimg_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m preprocess_input(img_array)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\n",
      "File \u001b[1;32mc:\\Users\\Nabu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\image_utils.py:324\u001b[0m, in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown data_format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Numpy array x has format (height, width, channel)\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# or (channel, height, width)\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# but original PIL image has format (width, height, channel)\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_first\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.02 MiB for an array with shape (299, 299, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess images function\n",
    "def preprocess_image(file_path):\n",
    "    img = load_img(file_path, target_size=(299, 299))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# Load and preprocess images for train and validation sets\n",
    "X_train = np.array([preprocess_image(file_path) for file_path in train_df['File_Path']])\n",
    "X_val = np.array([preprocess_image(file_path) for file_path in val_df['File_Path']])\n",
    "y_train = np.array(train_df['Label'])\n",
    "y_val = np.array(val_df['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0\n",
      "Type: GPU\n"
     ]
    }
   ],
   "source": [
    "# Check available GPUs\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Print information about each GPU\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name)\n",
    "        print(\"Type:\", gpu.device_type)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(299, 299, 3)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(38, activation='softmax')  # assuming you have 38 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('models/best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=16, callbacks=[checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save history object\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# Evaluation and prediction steps remain the same as in your original code\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history object\n",
    "with open('history[xception02042024].pkl', 'rb') as f:\n",
    "    loaded_history = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Evaluation of Model</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loaded_history['accuracy'], label='Training Accuracy')\n",
    "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(loaded_history['val_accuracy'], label='Validation Accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loaded_history['loss'], label='Training Loss')\n",
    "#plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(loaded_history['val_loss'], label='Validation Loss')\n",
    "#plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(test_gen):    \n",
    "    y_pred = []\n",
    "    error_list = []\n",
    "    error_pred_list = []\n",
    "    y_true = test_gen.labels\n",
    "    classes = list(test_gen.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    errors = 0\n",
    "    preds = model.predict(test_gen, verbose=1)\n",
    "    tests = len(preds)    \n",
    "    for i, p in enumerate(preds):        \n",
    "        pred_index = np.argmax(p)         \n",
    "        true_index = test_gen.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors = errors + 1\n",
    "            file = test_gen.filenames[i]\n",
    "            error_list.append(file)\n",
    "            error_class = classes[pred_index]\n",
    "            error_pred_list.append(error_class)\n",
    "        y_pred.append(pred_index)\n",
    "            \n",
    "    acc = (1 - errors/tests) * 100\n",
    "    msg = f'There were {errors} errors in {tests} tests for an accuracy of {acc:.2f}'\n",
    "    print(msg)\n",
    "    \n",
    "    ypred = np.array(y_pred)\n",
    "    ytrue = np.array(y_true)\n",
    "    f1score = f1_score(ytrue, ypred, average='weighted') * 100\n",
    "    if class_count <= 30:\n",
    "        cm = confusion_matrix(ytrue, ypred)\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count) + .5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count) + .5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    \n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    \n",
    "    return errors, tests, error_list, error_pred_list, f1score, y_pred, y_true\n",
    "\n",
    "errors, tests, error_list, error_pred_list, f1score, y_pred, y_true = predictor(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test generator\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "print('Test Loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class indices to class names\n",
    "class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "# Save class indices to a JSON file\n",
    "with open('class_indices_xception.json', 'w') as f:\n",
    "    json.dump(class_indices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Prediction of Disease</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict disease from an image\n",
    "def predict_disease(image_path):\n",
    "\n",
    "    img = load_img(image_path, target_size=(img_size))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    prediction = model.predict(img_array)[0]\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    disease = class_indices[str(predicted_class_index)]\n",
    "    confidence = prediction[predicted_class_index]\n",
    "\n",
    "    return disease, confidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class indices\n",
    "with open('class_indices_xception.json', 'r') as f:\n",
    "    class_indices = json.load(f)\n",
    "\n",
    "# Load remedies\n",
    "with open('remedies.json', 'r') as f:\n",
    "    remedies= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'D:\\testproject\\New Plant Diseases Dataset(Augmented)\\CornCommonRust3.JPG' \n",
    "\n",
    "disease, confidence = predict_disease(image_path)\n",
    "\n",
    "print(f'Predicted Disease: {disease}, Confidence: {confidence}')\n",
    "\n",
    "if disease.strip() in remedies:\n",
    "    remedy = remedies[disease]\n",
    "    print(f'Remedies: {remedy}')\n",
    "else: \n",
    "    print('No remedies found for this disease.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
