{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pickle #for saving history object\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history object\n",
    "with open('history[xception02042024].pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load history object\n",
    "with open('history[xception02042024].pkl', 'rb') as f:\n",
    "    loaded_history = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loaded_history['accuracy'], label='Training Accuracy')\n",
    "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(loaded_history['val_accuracy'], label='Validation Accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loaded_history['loss'], label='Training Loss')\n",
    "#plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(loaded_history['val_loss'], label='Validation Loss')\n",
    "#plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(test_gen):    \n",
    "    y_pred = []\n",
    "    error_list = []\n",
    "    error_pred_list = []\n",
    "    y_true = test_gen.labels\n",
    "    classes = list(test_gen.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    errors = 0\n",
    "    preds = model.predict(test_gen, verbose=1)\n",
    "    tests = len(preds)    \n",
    "    for i, p in enumerate(preds):        \n",
    "        pred_index = np.argmax(p)         \n",
    "        true_index = test_gen.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors = errors + 1\n",
    "            file = test_gen.filenames[i]\n",
    "            error_list.append(file)\n",
    "            error_class = classes[pred_index]\n",
    "            error_pred_list.append(error_class)\n",
    "        y_pred.append(pred_index)\n",
    "            \n",
    "    acc = (1 - errors/tests) * 100\n",
    "    msg = f'There were {errors} errors in {tests} tests for an accuracy of {acc:.2f}'\n",
    "    print(msg)\n",
    "    \n",
    "    ypred = np.array(y_pred)\n",
    "    ytrue = np.array(y_true)\n",
    "    f1score = f1_score(ytrue, ypred, average='weighted') * 100\n",
    "    if class_count <= 30:\n",
    "        cm = confusion_matrix(ytrue, ypred)\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count) + .5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count) + .5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    \n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    \n",
    "    return errors, tests, error_list, error_pred_list, f1score, y_pred, y_true\n",
    "\n",
    "errors, tests, error_list, error_pred_list, f1score, y_pred, y_true = predictor(test_generator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
